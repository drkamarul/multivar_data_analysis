# Linear Regression

1. A statistical method to model relationship between:

    - outcome: numerical variable.
    - predictors/independent variables: numerical, categorical variables.

2. A type of Generalized Linear Models (GLMs), which also includes other outcome types, e.g. categorical and count.

3. Basically, the linear relationship is structured as follows,

$$numerical\ outcome = numerical\ predictors + categorical\ predictors$$

## Simple linear regression (SLR)

### About SLR
1. Model _linear_ (straight line) relationship between:

    - outcome: numerical variable.
    - a predictor: numerical variable (only).
    
> _Note_: What if the predictor is a categorical variable? Remember, we already handled that with one-way ANOVA.

2. Formula,
$$numerical\ outcome = intercept + coefficient \times numerical\ predictor$$
in short,
$$\hat y = \beta_0 + \beta_1x_1$$
where $\hat y$ is the predicted value of the outcome y.

### Analysis

#### Libraries

```{r}
# library
library(foreign)
library(epiDisplay)
library(psych)
library(lattice)
library(rsq)
library(car)
library(broom)
library(tidyverse)
```

#### Data set

```{r}
# data
coronary = read.dta(here::here("data","coronary.dta"))
str(coronary)
```

### Data exploration

#### Descriptive statistics
```{r}
summ(coronary[c("chol", "dbp")])
```

#### Plots

```{r}
multi.hist(coronary[c("chol", "dbp")], ncol = 2)
par(mfrow = c(1, 2))
mapply(boxplot, coronary[c("chol", "dbp")], 
       main = colnames(coronary[c("chol", "dbp")]))
par(mfrow = c(1, 1))
```

### Univariable

Fit model,

```{r}
# model: chol ~ dbp
slr_chol = glm(chol ~ dbp, data = coronary)
summary(slr_chol)
Confint(slr_chol)  # 95% CI
```

Important results,

- Coefficient, $\beta$.
- 95% CI.
- _P_-value.

Obtain $R^2$, % of variance explained,

```{r}
rsq(slr_chol, adj = T)
```

Scatter plot,

```{r}
plot(chol ~ dbp, data = coronary)
abline(slr_chol)
```

this allows assessment of normality, linearity and equal variance assumptions. We expect eliptical/oval shape (normality), equal scatter of dots on both sides of the prediction line (equal variance). Both these indicate linear relationship between `chol` and `dbp`.

#### Interpretation
- 1mmHg increase in DBP causes 0.04mmol/L increase in cholestrol.
- DBP explains 17.6% variance in cholestrol.

#### Model equation
$$chol = 3.0 + 0.04\times dbp$$


## Multiple linear regression (MLR)

### About MLR
1. Model _linear_ relationship between:

    - outcome: numerical variable.
    - predictors: numerical, categorical variables.

> _Note_: MLR is a term that refers to linear regression with two or more _numerical_ variables. Whenever we have both numerical and categorical variables, the proper term for the regression model is _General Linear Model_. However, we will use the term MLR in this workshop.

2. Formula,
$$\begin{aligned}
numerical\ outcome = &\ intercept + coefficients \times numerical\ predictors \\
& + coefficients \times categorical\ predictors
\end{aligned}$$
in a shorter form,
$$\hat y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_kx_k$$
where we have _k_ predictors.

Whenever the predictor is a categorical variable with more than two levels, we use dummy variable(s). This can be easily specified in R using `factor()` if the variable is not yet properly specified as such. There is no problem with binary categorical variable.

For a categorical variable with more than two levels, the number of dummy variables (i.e. once turned into several binary variables) equals number of levels minus one. For example, whenever we have four levels, we will obtain three dummy (binary) variables.

### Analysis

#### Review data set
```{r}
# data
str(coronary)
```

We exclude `id`, `cad` and `age` from our data for the purpose of this analysis, keeping only `sbp , dbp, bmi, race` and `gender`. We will add `age` later in the exercise.

```{r}
coronary <- coronary %>% dplyr::select(-id, -cad, -age)
# remove id, cad, age from our data since we're not going to use them,
# easier to specifiy multivariable model.
```

#### Data exploration

##### Descriptive statistics
```{r}
summ(coronary[c("chol", "sbp", "dbp", "bmi")])
codebook(coronary[c("race", "gender")])
```

##### Plots
```{r}
plot(coronary)
multi.hist(coronary[c("chol", "sbp", "dbp", "bmi")])
par(mfrow = c(2, 2))
mapply(boxplot, coronary[c("chol", "sbp", "dbp", "bmi")], 
       main = colnames(coronary[c("chol", "sbp", "dbp", "bmi")]))
par(mfrow = c(1, 1))
par(mfrow = c(1, 2))
boxplot(chol ~ race, data = coronary)
boxplot(chol ~ gender, data = coronary)
par(mfrow = c(1, 1))
```

#### Variable selection

#### slr

We want to choose only variables with _P_-values < 0.25 to be included in MLR. Obtaining the _P_-values for each variable is easy by LR test,

```{r}
slr_chol0 = glm(chol ~ 1, data = coronary)
summary(slr_chol0)
```


##### Multivariable
Perform MLR with _all_ selected variables,
```{r}
# all
mlr_chol = glm(chol ~ sbp + dbp + bmi + race, data = coronary)
#mlr_chol = glm(chol ~ ., data = coronary)  # shortcut
summary(mlr_chol)
rsq(mlr_chol, adj = T)
```
Focus on,

- Coefficients, $\beta$s.
- 95% CI.
- _P_-values.

For model fit,

- $R^2$ -- % of variance explained by the model.
- Akaike Information Criterion, AIC -- for comparison with other models. This is not useful alone, but for comparison with other models. The model with the lowest AIC is the best model.


Looking at all these results, we choose: 

> `chol ~ dbp + race`

which has the lowest AIC.

```{r}
mlr_chol1 = glm(chol ~ dbp + race, data = coronary)
summary(mlr_chol1)
```



Our chosen model:

> `mlr_chol1: chol ~ dbp + race`
```{r}
summary(mlr_chol1)
Confint(mlr_chol1)  # 95% CI of the coefficients
```

Compare this model with the no-variable model and all-variable model by LR test and AIC comparison,
```{r}
# LR test
anova(slr_chol0, mlr_chol1, test = "LRT")  # sig. better than no var at all!
# model with no var at all is called Null Model
anova(mlr_chol, mlr_chol1, test = "LRT")  # no sig. dif with all vars model,
# model with 2 vars (dbp & race) is just as good as full model (with all the vars)
# model with all vars is called Saturated Model
```
```{r}
# AIC
AIC(slr_chol0, mlr_chol1, mlr_chol)
# our final model has the lowest AIC
```

### Multicollinearity, MC
Multicollinearity is the problem of repetitive/redundant variables -- high correlations between predictors. MC is checked by Variance Inflation Factor (VIF). VIF > 10 indicates MC problem.

```{r}
vif(mlr_chol1)  # all < 10
```

### Interaction, *

Interaction is the predictor variable combination that requires interpretation of regression coefficients separately based on the levels of the predictor (e.g. separate analysis for each race group, Malay vs Chinese vs Indian). This makes interpreting our analysis complicated. So, most of the time, we pray not to have interaction in our regression model.
```{r}
summary(glm(chol ~ dbp*race, data = coronary))  # dbp*race not sig.
# in R, it is easy to fit interaction by *
# dbp*race will automatically include all vars involved i.e. equal to
# glm(chol ~ dbp + race + dbp:race, data = coronary)
# use : to just include just the interaction
```
There is no interaction here because the included interaction term was insignificant.

## Model fit assessment: Residuals

### Histogram {-}
Raw residuals: Normality assumption.
```{r}
rraw_chol = resid(mlr_chol1)  # unstandardized
multi.hist(rraw_chol)
```

### Scatter plots {-}
Standardized residuals vs Standardized predicted values: Overall -- normality, linearity and equal variance assumptions.
```{r}
rstd_chol = rstandard(mlr_chol1)  # standardized residuals
pstd_chol = scale(predict(mlr_chol1))  # standardized predicted values
plot(rstd_chol ~ pstd_chol, xlab = "Std predicted", ylab = "Std residuals")
abline(0, 0)  # normal, linear, equal variance
```
The dots should form elliptical/oval shape (normality) and scattered roughly equal above and below the zero line (equal variance). Both these indicate linearity.

Raw residuals vs Numerical predictor by each predictors: Linearity assumption.
```{r}
plot(rraw_chol ~ coronary$dbp, xlab = "DBP", ylab = "Raw Residuals")
abline(0, 0)
```

## Interpretation
Now we have decided on our final model, rename the model,
```{r}
# rename the selected model
mlr_chol_final = mlr_chol1
```
and interpret the model,
```{r}
summary(mlr_chol_final)
Confint(mlr_chol_final)  # 95% CI of the coefficients
rsq(mlr_chol_final, adj = T)
```
- 1mmHg increase in DBP causes 0.03mmol/L increase in cholestrol, controlling for the effect of race.
- Being Chinese causes 0.36mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP.
- Being Indian causes 0.71mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP.
- DBP and race explains 22.3% variance in cholestrol.

Turn the results into data frames results using `broom`,
```{r}
tib_mlr = tidy(mlr_chol_final, conf.int = T); tib_mlr
```

Display the results using `kable` in a nice table,
```{r}
knitr::kable(tib_mlr)
```

We can export the results into a .csv file for use later (e.g. to prepare a table for journal articles etc.),
```{r}
write.csv(tib_mlr, "mlr_final.csv")
```

## Model equation
Cholestrol level in mmol/L can be predicted by its predictors as given by,
$$chol = 3.30 + 0.03\times dbp + 0.36\times race\ (chinese) + 0.71\times race\ (indian)$$

## Prediction
It is easy to predict in R using our fitted model above. First we view the predicted values for our sample,
```{r}
coronary$pred_chol = predict(mlr_chol_final)
head(coronary)
```

Now let us try predicting for any values for `dbp` and `race`,
```{r}
str(coronary[c("dbp", "race")])
# simple, dbp = 90, race = indian
predict(mlr_chol_final, list(dbp = 90, race = "indian"))
```

More data points
```{r}
new_data = data.frame(dbp = c(90, 90, 90), race = c("malay", "chinese", "indian"))
new_data
predict(mlr_chol_final, new_data)
new_data$pred_chol = predict(mlr_chol_final, new_data)
new_data
```

```{r}
detach("package:car", unload=TRUE)
detach("package:epiDisplay", unload=TRUE)
detach("package:psych", unload=TRUE)
detach("package:rsq", unload=TRUE)
detach("package:MASS", unload=TRUE)

```

