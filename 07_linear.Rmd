# Linear Regression

## Introduction

### Background
Linear regression is one of the most common statistical analyses in medical and health sciences. Linear regression models the linear (i.e. straight line) relationship between:

- **outcome**: numerical variable (e.g. blood pressure, BMI, cholesterol level).
- **predictors/independent variables**: numerical variables and categorical variables (e.g. gender, race, education level).

In simple words, we might be interested in knowing the relationship between the cholesterol level and its associated factors, for example gender, age, BMI and lifestyle. This can be explored by a linear regression analysis.

Linear regression is a type of Generalized Linear Models (GLMs), which also includes other outcome types, for example categorical and count. In subsequent chapters, we will cover these outcome types in form of logistic regression and Poisson regression. Basically, the relationship between the outcome and predictors in a linear regression is structured as follows,

$$\begin{aligned}
numerical\ outcome = &\ numerical\ predictors \\
& + categorical\ predictors
\end{aligned}$$

More appropriate forms of this relationship will explained later under simple and multiple linear regressions sections.

### Objectives
After completing this chapter, the readers are expected to

- understand the concept of simple and multiple linear regression 
- perform simple linear regression
- perform multiple linear regression
- perform model fit assessment of linear regression models
- present and interpret the results of linear regression analyses

## Prepare R Environment for Analysis

### Libraries
For this chapter, we will be using the following packages:

- **foreign**: for reading SPSS and STATA datasets
- **tidyverse**: a general and powerful package for data transformation
- **psych**: for descriptive statistics
- **gtsummary**, **ggplot2**, **ggpubr**, **GGally**: for plotting the graphs
- **rsq**: for getting $R^2$ value from a GLM model
- **broom**: for tidying up the results
- **car**: for `vif()` function

These are loaded as follows using the function `library()`,
```{r, message=FALSE, error=FALSE}
# library
library(foreign)
library(tidyverse)
library(psych)
library(gtsummary)
library(ggplot2)
library(ggpubr)
library(GGally)
library(rsq)
library(broom)
library(car)
```

### Data set
We will use the `coronary.dta` dataset in STATA format. The dataset contains the total cholesterol level, their individual characteristics and intervention groups in a hypothetical clinical trial. The dataset contains 200 observations for nine variables:

1. _id_: Subjects' ID.
2. _cad_: Coronary artery disease status (categorical) {no cad, cad}.
2. _sbp_ : Systolic blood pressure in mmHg (numerical).
2. _chol_: Total cholesterol level in mmol/L (numerical).
2. _age_: Age in years (numerical).
2. _bmi_: Body mass index (numerical).
2. _race_: Race of the subjects (categorical) {malay, chinese, indian}.
2. _gender_: Gender of the subjects (categorical) {woman, man}

The dataset is loaded as follows,
```{r}
# data
coronary = read.dta("data/coronary.dta")
```

We then look at the basic structure of the dataset,
```{r}
str(coronary)
```

## Simple Linear Regression (SLR)

### About SLR
SLR models _linear_ (straight line) relationship between:

- **outcome**: numerical variable.
- **ONE predictor**: numerical/categorical variable.
    
_Note_: When the predictor is a categorical variable, this is typically analyzed by one-way ANOVA. However, SLR can also handle a categorical variable in the GLM framework.

We may formally represent SLR in form of an equation as follows,

$$numerical\ outcome = intercept + coefficient \times predictor$$
or in a shorter form using mathematical notations,

$$\hat y = \beta_0 + \beta_1x_1$$
where $\hat y$ (pronounced y hat) is the predicted value of the outcome y.

### Data exploration
Let say, for the SLR we are interested in knowing whether diastolic blood pressure (predictor) is associated with the cholesterol level (outcome). We explore the variables by obtaining the descriptive statistics and plotting the data distribution.

We obtain the descriptive statistics of the variables,
```{r, message=FALSE}
coronary %>% select(chol, dbp) %>% describe()
```
and the histograms and box-and-whiskers plots,
```{r, message=FALSE}
hist_chol = ggplot(coronary, aes(chol)) + 
  geom_histogram(color = "blue", fill = "white")
hist_dbp = ggplot(coronary, aes(dbp)) + 
  geom_histogram(color = "red", fill = "white")
bplot_chol = ggplot(coronary, aes(chol)) + 
  geom_boxplot(color = "blue", )
bplot_dbp = ggplot(coronary, aes(dbp)) + 
  geom_boxplot(color = "red")
ggarrange(hist_chol, bplot_chol, hist_dbp, bplot_dbp)
```

### Univariable analysis
For the analysis, we fit the SLR model, which consists of only one predictor (univariable). Here, `chol` is specified as the outcome, and `dbp` as the predictor. In `glm`, the formula is specified as `outcome ~ predictor`. Here, we specify `chol ~ dbp` as the formula in `glm`.

We fit and view the summary information of the model as,
```{r}
slr_chol = glm(chol ~ dbp, data = coronary)
summary(slr_chol)
```

We can tidy up the glm output and obtain the 95% confidence interval (CI) using `tidy()` from the `broom` package,
```{r}
tidy(slr_chol, conf.int = TRUE)
```

From the output above, we pay attention at these results:

- coefficients, $\hat \beta$ -- column `term`.
- 95% CI -- columns `conf.low` and `conf.high`.
- _P_-value -- column `p.value`.

### Model fit assessment
It is important to assess to what extend the SLR model reflects the data. First, we can assess this by $R^2$, which is the percentage of the variance for the outcome that is explained by the predictor. In simpler words, to what extend the variation in the values of the outcome is caused/explained by the predictor. This ranges from 0% (the predictor does not explain the outcome at all) to 100% (the predictor explains the outcome perfectly). Here, we obtain the $R^2$ values,
```{r}
rsq(slr_chol)
```
  
Next, we can assess the model fit by a scatter plot,
```{r, message=FALSE}
plot_slr = ggplot(coronary, aes(x = dbp, y = chol)) + 
  geom_point() + geom_smooth(method = lm)
plot_slr
```
This plot allows the assessment of normality, linearity and equal variance assumptions. We expect an elliptical/oval shape (normality) and equal scatter of dots above and below the prediction line (equal variance). These aspects indicate a linear relationship between `chol` and `dbp` (linearity).

### Presentation and interpretation
To present the result, we can use `tbl_regression()` to come up with a nice table. We use `slr_chol` of the `glm` output with `tbl_regression()` in the `gtsummary` package.
```{r, error=FALSE, message=FALSE}
tbl_regression(slr_chol, intercept = TRUE)
```
Here, we use `intercept = TRUE` to include the intercept value in the table. By default, this is omitted by the `tbl_regression()`.

It is also very informative to present the model equation,

$$chol = 3.0 + 0.04\times dbp$$

Based on the $R^2$ (which was `r round(rsq(slr_chol), 3)`), table and model equation, we may interpret the results as follows:

- 1mmHg increase in DBP causes 0.04mmol/L increase in cholesterol level.
- DBP explains `r round(rsq(slr_chol)*100, 1)`% of the variance in cholesterol level.

## Multiple Linear Regression (MLR)

### About MLR
MLR models _linear_ relationship between:

- **outcome**: numerical variable.
- **MORE than one predictors**: numerical and categorical variables.

We may formally represent MLR in form of an equation,

$$\begin{aligned}
numerical\ outcome = &\ intercept \\
& + coefficients \times numerical\ predictors \\
& + coefficients \times categorical\ predictors
\end{aligned}$$
or in a shorter form,

$$\hat y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p$$
where we have _p_ predictors.

Whenever the predictor is a categorical variable with more than two levels, we use dummy variable(s). There is no issue with binary categorical variable. For the variable with more than two levels, the number of dummy variables (i.e. once turned into several binary variables) equals number of levels minus one. For example, whenever we have four levels, we will obtain three dummy (binary) variables. As we will see later, `glm` will automatically do this for `factor` variable and provide separate estimates for each dummy variable.

### Data exploration
Now, for the MLR we are no longer restricted to one predictor. Let say, we are interested in knowing the relationship between blood pressure (SBP and DBP), age, BMI, race and render as the predictors and the cholesterol level (outcome). As before, we explore the variables by the descriptive statistics,
```{r, message=FALSE}
# numerical
coronary %>% select(-id, -cad, -race, -gender) %>% describe()
# categorical
coronary %>% select(race, gender) %>% tbl_summary()
```
and the pairs plot, where we focus on the distribution of the data by histograms and box-and-whiskers plots. The pairs plot also includes information on the bivariate correlation statistics between the numerical variables.
```{r, message=FALSE}
coronary %>% select(-id, -cad) %>% ggpairs()
```

### Univariable -- Variable selection
For the univariable analysis in the context of MLR, we aim to select variables that are worthwhile to be included in the multivariable model.

In the context of **exploratory research**, we want to choose only variables with _P_-values < 0.25 to be included in MLR. To obtain the _P_-values, you may perform separate SLRs for each of the predictors (on your own). However, obtaining _P_-value for each predictor is easy by `add1()` function. Here, we use likelihood ratio test (LRT) using `test = "LRT"` option to obtain the _P_-values. We start with an intercept only model `slr_chol0` using `chol ~ 1` formula specification in the `glm` followed by `add1()`. `add1()` will test each predictor one by one.
```{r}
slr_chol0 = glm(chol ~ 1, data = coronary)  # intercept only model
add1(slr_chol0, scope = ~ sbp + dbp + age + bmi + race + gender, 
     test = "LRT")
```
From the output, all variables are significant with _P_ < .25 except `gender`. These variables, excluding `gender`, are candidates in this variable selection step.

However, please keep in mind that in the context of **confirmatory research**, the variables that we want to include are not merely based on _P_-values alone. It is important to consider expert judgement as well.

### Multivariable -- Judicious variable selection
Multivariable analysis involves more than one predictors. In the univariable variable selection, we decided on several potential predictors. For MLR, we (judiciously) included these variables in an MLR model. In the present dataset, we have the following considerations:

- including both SBP and DBP is redundant, because both represent the blood pressure. These variables are also highly correlated. This is indicated by the correlation value, _r_ = 0.828 and scatter plot for the SBP-DBP pair in the pairs plot in the data exploration step.
- `gender` was not sigficant, thus we may exclude the variable.
- let say, as advised by experts in the field, we should exclude `age` in the modelling.

Now, given these considerations, we perform MLR with the selected variables,
```{r}
mlr_chol = glm(chol ~ dbp + bmi + race, data = coronary)
summary(mlr_chol)
```

From the output above, for each variable, we focus these results:

- coefficients, $\hat \beta$s -- column `term`.
- 95% CIs -- columns `conf.low` and `conf.high`.
- _P_-values -- column `p.value`.

Note that for a categorical variable with more than two categories, the estimates are obtained for each dummy variable. In our case, `race` consists of Malay, Chinese and Indian. From the output, the dummy variables are `racechinese` representing Chinese vs Malay and `raceindian` representing Indian vs Malay dummy variables, where Malay is set as the baseline comparison group.

We also notice that some variables are not significant at significance level of 0.05, namely `bmi` and `racechinese`. As for `racechinese` dummy variable, because this forms part of the `race` variable, we accept the variable because it is marginally insignificant (0.0512 vs 0.05) and the other dummy variable `raceindian` is significant.

#### Stepwise automatic variable selection {-}
We noted that not all variables included in the model are significant. In our case, we may remove `bmi` because it is not statistically significant. But in exploratory research where we have hundreds of variables, it is impossible to select variables by eye-ball judgement. So, in this case, how to perform the variable selection? To explore the significance of the variables, we may perform stepwise automatic selection. It is important to know stepwise selection is meant for exploratory research. For confirmatory analysis, it is important to rely on expert opinion for the variable selection. We may perform forward, backward or both forward and backward selection combined.

_Forward selection_ starts with an intercept only or empty model without variable. It proceeds by adding one variable after another. In R, Akaike information criterion (AIC) is used as the comparative goodness-of-fit measure and model quality. In the stepwise selection, it seeks to find the model with the lowest AIC iteratively and the steps are shown in the output. More information about AIC can be referred to @hu2007 and @aic-wiki.
```{r}
# forward
mlr_chol_stepforward = step(slr_chol0, scope = ~ dbp + bmi + race, 
                            direction = "forward")
```

_Backward selection_ starts with a model containing all variables. Then, it proceeds by removing one variable after another, of which it aims to find the model with the lowest AIC.
```{r}
# backward
mlr_chol_stepback = step(mlr_chol, direction = "backward")
```

_Bidirectional selection_, as implemented in R, starts as with the model with all variables. Then, it proceeds with removing or adding variables, which combines both forward and backward selection methods. It stops once it finds the model with the lowest AIC>
```{r}
# both
mlr_chol_stepboth = step(mlr_chol, direction = "both")
```

#### Preliminary model {-}
Let say, after considering the _P_-value, stepwise selection (in exploratory research) and expert opinion, we decided that our preliminary model is,

`chol ~ dbp + race`

and we fit the model again to view basic information of the model,
```{r, message=FALSE}
mlr_chol_sel = glm(chol ~ dbp + race, data = coronary)
summary(mlr_chol_sel)
rsq(mlr_chol_sel)
```

### Interaction
Interaction is the combination of predictors that requires interpretation of their regression coefficients separately based on the levels of the predictor, for example it needs separate interpretation for each race group, Malay vs Chinese vs Indian. This makes interpreting our analysis complicated. So, most of the time, we pray not to have interaction in our regression model.
```{r}
summary(glm(chol ~ dbp * race, data = coronary))
```
From the output, there is no evidence that suggests the presence of interaction because the included interaction term was insignificant. In R, it is easy to fit interaction by `*`, e.g. `dbp * race` will automatically includes all variables involved. This is equal to specifiying `glm(chol ~ dbp + race + dbp:race, data = coronary)`, where we can use `:` to include the interaction.

### Model fit assessment
For MLR, we assess the model fit by $R^2$ and histogram and scatter plots of residuals. Residuals, in simple term, are the discrepancies between the observed values (dots) and the predicted values (by the fit MLR model). So, the lesser the discrepancies, the better is the model fit.

#### Percentage of variance explained, $R^2$ {-}
First, we obtain the $R^2$ values. In comparison to the $R^2$ obtained for the SLR, we include `adj = TRUE` here to obtain an adjusted $R^2$. The adjusted $R^2$ here is the $R^2$ with penalty for the number of predictors _p_. This discourages including too many variables, which might be unnecessary.
```{r}
rsq(mlr_chol_sel, adj = TRUE)
```

#### Histogram and box-and-whiskers plot {-}
Second, we plot a histogram and a box-and-whiskers plot to assess the normality of raw/unstandardized residuals of the MLR model. We expect normally distributed residuals to indicate a good fit of the MLR model. Here, we have a normally distributed residuals.
```{r}
rraw_chol = resid(mlr_chol_sel)
hist(rraw_chol)
boxplot(rraw_chol)
```

#### Scatter plots {-}
Third, we plot a standardized residuals (Y-axis) vs standardized predicted values (X-axis). Similar to the one for SLR, this plot allows assessment of normality, linearity and equal variance assumptions. The dots should form elliptical/oval shape (normality) and scattered roughly equal above and below the zero line (equal variance). Both these indicate linearity. Our plot below shows that the assumptions are met.
```{r}
rstd_chol = rstandard(mlr_chol_sel)  # standardized residuals
pstd_chol = scale(predict(mlr_chol_sel))  # standardized predicted values
plot(rstd_chol ~ pstd_chol, xlab = "Std predicted", ylab = "Std residuals")
abline(0, 0)  # normal, linear, equal variance
```

In addition to the standardized residuals vs standardized predicted values plot, for numerical predictors, we assess the linear relationship between the raw residuals and the observed values of the numerical predictors. We plot the raw residuals vs numerical predictor below. The plot interpreted in similar way to the standardized residuals vs standardized predicted values plot. The plot shows good linearity between the residuals and the numerical predictor.
```{r}
plot(rraw_chol ~ coronary$dbp, xlab = "DBP", ylab = "Raw Residuals")
abline(0, 0)
```

### Presentation and Interpretation
After passing all the assumption checks, we may now decide on our final model. We may rename the preliminary model `mlr_chol_sel` to `mlr_chol_final` for easier reference. 
```{r}
mlr_chol_final = mlr_chol_sel
```

Similar to SLR, we use `tbl_regression()` to come up with a nice table to present the results.
```{r, message=FALSE}
tbl_regression(mlr_chol_final, intercept = TRUE)
```

It will be useful to be able to save the output in the spreadsheet format for later use. We can use `tidy()` function in this case and export it to a .csv file,
```{r, eval=FALSE}
tib_mlr = tidy(mlr_chol_final, conf.int = TRUE)
write.csv(tib_mlr, "mlr_final.csv")
```

Then, we present the model equation. Cholestrol level in mmol/L can be predicted by its predictors as given by,

$$chol = 3.30 + 0.03\times dbp + 0.36\times race\ (chinese) + 0.71\times race\ (indian)$$
Based on the adjusted $R^2$, table and model equation, we may interpret the results as follows:

- 1mmHg increase in DBP causes 0.03mmol/L increase in cholestrol, controlling for the effect of race.
- Being Chinese causes 0.36mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP.
- Being Indian causes 0.71mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP.
- DBP and race explains 22.3% variance in cholestrol.

## Prediction
In some situations, it is useful to use the SLR/MLR model for prediction. For example, we may want to predict the cholesterol level of a patient given some clinical characteristics. We can use the final model above for prediction. For starter, let us view the predicted values for our sample,
```{r}
coronary$pred_chol = predict(mlr_chol_final)
head(coronary)
```
Compare the predicted values with the observed cholesterol level. Recall that we already checked this for the model fit assessment before.

It is more useful to predict for newly observed data. Let us try predicting the cholesterol level for an Indian patient with DBP = 90mmHg,
```{r}
predict(mlr_chol_final, list(dbp = 90, race = "indian"))
```

Now, we also do so the data with many more patients,
```{r}
new_data = data.frame(dbp = c(90, 90, 90), 
                      race = c("malay", "chinese", "indian"))
predict(mlr_chol_final, new_data)
new_data$pred_chol = predict(mlr_chol_final, new_data)
new_data
```

## Summary
In this chapter, we went through the basics about SLR and MLR. We performed the analysis for each and learned how to assess the model fit for the regression models. We learned how to nicely present and interpret the results. In addition, we also learned how to utilize the model for prediction.
